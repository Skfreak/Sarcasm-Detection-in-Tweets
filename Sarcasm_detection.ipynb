{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sir,\n",
    "    As you asked are not providing the GloVe Model since it is over 2Gb. Rest of the things are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193514, 200)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking the .txt file that contains model\n",
    "glove_input_file = './glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "word2vec_output_file = 'twitter.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the Stanford GloVe model trained on twitter dataset\n",
    "filename = 'twitter.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Dataset/SemEval2018-T3-train-taskA.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      1  Sweet United Nations video. Just in time for C...\n",
       "1   2      1  @mrdahl87 We are rumored to have talked to Erv...\n",
       "2   3      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
       "3   4      0                3 episodes left I'm dying over here\n",
       "4   5      1  I can't breathe! was chosen as the most notabl..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1916\n",
       "1    1901\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if data is balanced\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['id'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning \n",
    "\n",
    "1.Remove URLs <br>\n",
    "2.Remove usernames (mentions)<br>\n",
    "3.Remove hashtags<br>\n",
    "4.Remove special characters<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Removing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're never too old for Footie Pajamas. http://t.co/ElzGqsX2yQ\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will first take just the data\n",
    "\n",
    "data = df['tweet']\n",
    "df['tweet'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cheching by removing url in one row\n",
    "result = re.sub(r\"http\\S+\", \"\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing urls from entire dataset\n",
    "dt = []                                            #new list\n",
    "for result in data:                                #running loop for full dataset\n",
    "    result = re.sub(r\"http\\S+\", \"\", result)         \n",
    "    dt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.asanyarray(dt)\n",
    "df['tweet'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      1  Sweet United Nations video. Just in time for C...\n",
       "1      1  @mrdahl87 We are rumored to have talked to Erv...\n",
       "2      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
       "3      0                3 episodes left I'm dying over here\n",
       "4      1  I can't breathe! was chosen as the most notabl..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will first take just the data\n",
    "\n",
    "data = df['tweet']\n",
    "df['tweet'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing usernames from entire dataset\n",
    "dt = []                                            #new list\n",
    "for result in data:                                #running loop for full dataset\n",
    "    result = re.sub(r\"@[^\\s]+[\\s]?\", \"\", result)         \n",
    "    dt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>We are rumored to have talked to Erv's agent.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      1  Sweet United Nations video. Just in time for C...\n",
       "1      1  We are rumored to have talked to Erv's agent.....\n",
       "2      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
       "3      0                3 episodes left I'm dying over here\n",
       "4      1  I can't breathe! was chosen as the most notabl..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.asanyarray(dt)\n",
    "df['tweet'] = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will first take just the data\n",
    "\n",
    "data = df['tweet']\n",
    "df['tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing usernames from entire dataset\n",
    "dt = []                                            #new list\n",
    "for result in data:                                #running loop for full dataset\n",
    "    result = re.sub(r\"#[^\\s]+[\\s]?\", \"\", result)         \n",
    "    dt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>We are rumored to have talked to Erv's agent.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      1  Sweet United Nations video. Just in time for C...\n",
       "1      1  We are rumored to have talked to Erv's agent.....\n",
       "2      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
       "3      0                3 episodes left I'm dying over here\n",
       "4      1  I can't breathe! was chosen as the most notabl..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.asanyarray(dt)\n",
    "df['tweet'] = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will first take just the data\n",
    "\n",
    "data = df['tweet']\n",
    "df['tweet'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for remove in map(lambda r: re.compile(re.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                                                                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                     \"!\", \"?\", \".\", \"'\",\n",
    "                                                                     \"--\", \"---\", \"#\"]):\n",
    "            df.loc[:, \"tweet\"].replace(remove, \"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are rumored to have talked to Ervs agent and the Angels asked about Ed Escobar thats hardly nothing    '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./cleaned_data/converted_after_special_chars.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now doing word2vec embeddings for the data and appending the column that represents the overall word2vec of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(df['tweet'])):\n",
    "    temp = nltk.word_tokenize(df['tweet'][i])\n",
    "    temp = np.asarray(temp)\n",
    "    tokens.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sweet united nations video just in time for ch...</td>\n",
       "      <td>[sweet, united, nations, video, just, in, time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>we are rumored to have talked to ervs agent an...</td>\n",
       "      <td>[we, are, rumored, to, have, talked, to, ervs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>hey there nice to see you minnesotand winter w...</td>\n",
       "      <td>[hey, there, nice, to, see, you, minnesotand, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left im dying over here</td>\n",
       "      <td>[3, episodes, left, im, dying, over, here]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i cant breathe was chosen as the most notable ...</td>\n",
       "      <td>[i, cant, breathe, was, chosen, as, the, most,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  \\\n",
       "0      1  sweet united nations video just in time for ch...   \n",
       "1      1  we are rumored to have talked to ervs agent an...   \n",
       "2      1  hey there nice to see you minnesotand winter w...   \n",
       "3      0                 3 episodes left im dying over here   \n",
       "4      1  i cant breathe was chosen as the most notable ...   \n",
       "\n",
       "                                                text  \n",
       "0  [sweet, united, nations, video, just, in, time...  \n",
       "1  [we, are, rumored, to, have, talked, to, ervs,...  \n",
       "2  [hey, there, nice, to, see, you, minnesotand, ...  \n",
       "3         [3, episodes, left, im, dying, over, here]  \n",
       "4  [i, cant, breathe, was, chosen, as, the, most,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokens = np.asarray(tokens)\n",
    "df['text'] = tokens\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building word2vec representation for every sentence by taking the mean of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mean_wv = []\n",
    "#i=0\n",
    "length = len(model.word_vec('hello'))\n",
    "for sentence in df['text']:\n",
    "    #i = i + 1\n",
    "    l = []\n",
    "    for w in sentence:\n",
    "        if w in model.wv:\n",
    "            temp = model.word_vec(w)\n",
    "        else:\n",
    "            temp = np.zeros(length)\n",
    "        l.append(temp)   \n",
    "        \n",
    "    l = np.asarray(l)\n",
    "    #mean_wv.append(l)\n",
    "    if len(l) == 0:\n",
    "        m = np.zeros(length)\n",
    "    else:\n",
    "        m = np.mean(l, axis = 0)\n",
    "    \n",
    "    #print(len(l))\n",
    "    mean_wv.append(m)\n",
    "\n",
    "mean_wv = np.asarray(mean_wv)\n",
    "    \n",
    "#i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3817, 200)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['mean'] = mean_wv.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sweet united nations video just in time for ch...</td>\n",
       "      <td>[sweet, united, nations, video, just, in, time...</td>\n",
       "      <td>[0.009712891653180122, 0.0635509341955185, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>we are rumored to have talked to ervs agent an...</td>\n",
       "      <td>[we, are, rumored, to, have, talked, to, ervs,...</td>\n",
       "      <td>[0.009882523531192228, 0.1374173654537452, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>hey there nice to see you minnesotand winter w...</td>\n",
       "      <td>[hey, there, nice, to, see, you, minnesotand, ...</td>\n",
       "      <td>[0.08628474579503138, 0.17179111225737465, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left im dying over here</td>\n",
       "      <td>[3, episodes, left, im, dying, over, here]</td>\n",
       "      <td>[0.18709842727652617, -0.023326572562967027, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i cant breathe was chosen as the most notable ...</td>\n",
       "      <td>[i, cant, breathe, was, chosen, as, the, most,...</td>\n",
       "      <td>[0.003277106210589409, 0.19845840334892273, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  \\\n",
       "0      1  sweet united nations video just in time for ch...   \n",
       "1      1  we are rumored to have talked to ervs agent an...   \n",
       "2      1  hey there nice to see you minnesotand winter w...   \n",
       "3      0                 3 episodes left im dying over here   \n",
       "4      1  i cant breathe was chosen as the most notable ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  [sweet, united, nations, video, just, in, time...   \n",
       "1  [we, are, rumored, to, have, talked, to, ervs,...   \n",
       "2  [hey, there, nice, to, see, you, minnesotand, ...   \n",
       "3         [3, episodes, left, im, dying, over, here]   \n",
       "4  [i, cant, breathe, was, chosen, as, the, most,...   \n",
       "\n",
       "                                                mean  \n",
       "0  [0.009712891653180122, 0.0635509341955185, 0.1...  \n",
       "1  [0.009882523531192228, 0.1374173654537452, 0.0...  \n",
       "2  [0.08628474579503138, 0.17179111225737465, -0....  \n",
       "3  [0.18709842727652617, -0.023326572562967027, -...  \n",
       "4  [0.003277106210589409, 0.19845840334892273, 0....  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the features as the mean vectors we got and then training on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = data[:,:1]\n",
    "X = data[:,3:]\n",
    "Y=Y.astype('int')\n",
    "\n",
    "# getting proper X\n",
    "y = []\n",
    "for i in range(len(X)):\n",
    "    y.append(X[i][0])\n",
    "\n",
    "y = np.asarray(y)\n",
    "\n",
    "X = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using poly kernel : 0.49345549738219896\n",
      "Accuracy using rbf kernel : 0.6151832460732984\n"
     ]
    }
   ],
   "source": [
    "for kernel in ('poly', 'rbf'):\n",
    "    clf = SVC(kernel=kernel)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print('Accuracy using ' + kernel + ' kernel : ' + str(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.6151832460732984\n",
      "\n",
      "Confusion Matrix : \n",
      "[[265 112]\n",
      " [182 205]]\n",
      "\n",
      "Precission Score :  0.6466876971608833\n",
      "\n",
      "Recall Score :  0.5297157622739018\n",
      "\n",
      "F1 Score : \n",
      "0.5823863636363636\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy : ', acc)\n",
    "\n",
    "cnf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('\\nConfusion Matrix : ')\n",
    "print(cnf)\n",
    "\n",
    "print('\\nPrecission Score : ', precision_score(y_test, y_pred))\n",
    "\n",
    "print('\\nRecall Score : ', recall_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('\\nF1 Score : ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sweet united nations video just in time for ch...</td>\n",
       "      <td>[sweet, united, nations, video, just, in, time...</td>\n",
       "      <td>[0.009712891653180122, 0.0635509341955185, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>we are rumored to have talked to ervs agent an...</td>\n",
       "      <td>[we, are, rumored, to, have, talked, to, ervs,...</td>\n",
       "      <td>[0.009882523531192228, 0.1374173654537452, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>hey there nice to see you minnesotand winter w...</td>\n",
       "      <td>[hey, there, nice, to, see, you, minnesotand, ...</td>\n",
       "      <td>[0.08628474579503138, 0.17179111225737465, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left im dying over here</td>\n",
       "      <td>[3, episodes, left, im, dying, over, here]</td>\n",
       "      <td>[0.18709842727652617, -0.023326572562967027, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i cant breathe was chosen as the most notable ...</td>\n",
       "      <td>[i, cant, breathe, was, chosen, as, the, most,...</td>\n",
       "      <td>[0.003277106210589409, 0.19845840334892273, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  \\\n",
       "0      1  sweet united nations video just in time for ch...   \n",
       "1      1  we are rumored to have talked to ervs agent an...   \n",
       "2      1  hey there nice to see you minnesotand winter w...   \n",
       "3      0                 3 episodes left im dying over here   \n",
       "4      1  i cant breathe was chosen as the most notable ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  [sweet, united, nations, video, just, in, time...   \n",
       "1  [we, are, rumored, to, have, talked, to, ervs,...   \n",
       "2  [hey, there, nice, to, see, you, minnesotand, ...   \n",
       "3         [3, episodes, left, im, dying, over, here]   \n",
       "4  [i, cant, breathe, was, chosen, as, the, most,...   \n",
       "\n",
       "                                                mean  \n",
       "0  [0.009712891653180122, 0.0635509341955185, 0.1...  \n",
       "1  [0.009882523531192228, 0.1374173654537452, 0.0...  \n",
       "2  [0.08628474579503138, 0.17179111225737465, -0....  \n",
       "3  [0.18709842727652617, -0.023326572562967027, -...  \n",
       "4  [0.003277106210589409, 0.19845840334892273, 0....  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### just checking for sentences with less than or equal to 1 (is commented below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in df.index.values:\n",
    "#     if(len(df.text[i]) <= 1):\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trying more sofisticated approach\n",
    "\n",
    "1. first apply window approach and get max and min distance\n",
    "2. use best and worst similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# applying max and min\n",
    "final_vect = []\n",
    "dist_max = []\n",
    "dist_min = []\n",
    "\n",
    "for sent in df.text:\n",
    "    if(len(sent) == 0):\n",
    "        final_vect.append(np.zeros(200))\n",
    "        dist_max.append(0)\n",
    "        dist_min.append(0)\n",
    "    elif(len(sent) == 1):\n",
    "        if(sent[0] in model.wv):\n",
    "            temp = model.get_vector(sent[0])\n",
    "            final_vect.append(temp)\n",
    "            dist_min.append(np.linalg.norm(temp))\n",
    "            dist_max.append(np.linalg.norm(temp))\n",
    "        else:\n",
    "            final_vect.append(np.zeros(200))\n",
    "            dist_max.append(0)\n",
    "            dist_min.append(0)\n",
    "    else:\n",
    "        w = np.zeros(200)\n",
    "        w1 = np.zeros(200)\n",
    "        i = 1\n",
    "        \n",
    "        while(i< len(sent)):\n",
    "            c1 = 0\n",
    "            c2 = 0\n",
    "            v1 = np.zeros(200)\n",
    "            v2 = np.zeros(200)\n",
    "            \n",
    "            for j in range(i):\n",
    "                if(sent[j] in model.wv):\n",
    "                    v1 += model.wv[sent[j]]\n",
    "                    c1 += 1\n",
    "                else:\n",
    "                    v1 += np.zeros(200)\n",
    "                    c1 += 1\n",
    "            \n",
    "            k = i\n",
    "            while k < len(sent):\n",
    "                if(sent[k] in model.wv):\n",
    "                    v2 += model.wv[sent[k]]\n",
    "                    k += 1\n",
    "                    c2 += 1\n",
    "                else:\n",
    "                    v2 += np.zeros(200)\n",
    "                    k += 1\n",
    "                    c2 += 1\n",
    "            v1 = v1 / c1\n",
    "            v2 = v2 / c2\n",
    "\n",
    "            if i == 1:\n",
    "                w = v1 - v2\n",
    "                w1 = w\n",
    "            else:\n",
    "                if(np.linalg.norm(w) < np.linalg.norm(v1 - v2)):\n",
    "                    w = v1 - v2 \n",
    "                else:\n",
    "                    w1 = v1 - v2\n",
    "                    \n",
    "            #print(i)\n",
    "            i += 1\n",
    "        final_vect.append(w)\n",
    "        dist_max.append(np.linalg.norm(w))\n",
    "        dist_min.append(np.linalg.norm(w1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now final_vect has the max distant vectors and dist_max and dist_min have the maximum and minimum distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting the lists to arrays\n",
    "final_features = np.asarray(final_vect)\n",
    "max_dist = np.asarray(dist_max).reshape(-1,1)\n",
    "min_dist = np.asarray(dist_min).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  stacking above calculated features together\n",
    "\n",
    "X = np.hstack((final_features, max_dist, min_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.5388888888888889\n",
      "\n",
      "Confusion Matrix : \n",
      "[[390 233]\n",
      " [348 289]]\n",
      "\n",
      "Precission Score :  0.553639846743295\n",
      "\n",
      "Recall Score :  0.45368916797488223\n",
      "\n",
      "F1 Score : \n",
      "0.4987057808455565\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy : ', acc)\n",
    "\n",
    "cnf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('\\nConfusion Matrix : ')\n",
    "print(cnf)\n",
    "\n",
    "print('\\nPrecission Score : ', precision_score(y_test, y_pred))\n",
    "\n",
    "print('\\nRecall Score : ', recall_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('\\nF1 Score : ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now try to use the similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# applying max and min\n",
    "best_sim = []\n",
    "sec_best_sim = []\n",
    "\n",
    "worst_sim = []\n",
    "sec_worst_sim = []\n",
    "\n",
    "for sent in df.text:\n",
    "    if(len(sent) == 0):\n",
    "        \n",
    "        best_sim.append(0)\n",
    "        sec_best_sim.append(0)\n",
    "        \n",
    "        worst_sim.append(0)\n",
    "        sec_worst_sim.append(0)\n",
    "        \n",
    "    elif(len(sent) == 1):\n",
    "        if(sent[0] in model.wv):                         #if the word in vocab            \n",
    "            best_sim.append(1)\n",
    "            sec_best_sim.append(1)\n",
    "            \n",
    "            worst_sim.append(1)\n",
    "            sec_worst_sim.append(1)\n",
    "        else:                                            #if word not in vocab\n",
    "            best_sim.append(1)\n",
    "            sec_best_sim.append(1)\n",
    "            \n",
    "            worst_sim.append(1)\n",
    "            sec_worst_sim.append(1)\n",
    "    else:\n",
    "        i = 0\n",
    "        max1 = 0\n",
    "        max2 = 0\n",
    "        min1 = 1\n",
    "        min2 = 1\n",
    "        while(i< len(sent) - 1):\n",
    "            j = i + 1\n",
    "            while( j < len(sent) ):\n",
    "                if( sent[i] in model.wv and sent[j] in model.wv ):\n",
    "                    temp = model.similarity(sent[i], sent[j])\n",
    "                    if(temp > max1):\n",
    "                        max2 = max1\n",
    "                        max1 = temp\n",
    "                    if(temp < min1):\n",
    "                        min2 = min1\n",
    "                        min1 = temp\n",
    "                \n",
    "                j += 1\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "           \n",
    "        best_sim.append(max1)\n",
    "        sec_best_sim.append(max2)\n",
    "        \n",
    "        worst_sim.append(min1)\n",
    "        sec_worst_sim.append(min2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting lists to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_sim = np.array(best_sim).reshape(-1,1)\n",
    "sec_best_sim = np.array(sec_best_sim).reshape(-1,1)\n",
    "\n",
    "worst_sim = np.array(worst_sim).reshape(-1,1)\n",
    "sec_worst_sim = np.array(sec_worst_sim).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  stacking above calculated features together\n",
    "\n",
    "X = np.hstack((best_sim, sec_best_sim, worst_sim, sec_worst_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C = 10)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.5274869109947644\n",
      "\n",
      "Precission Score :  0.5200642054574639\n",
      "\n",
      "Recall Score :  0.8393782383419689\n",
      "\n",
      "F1 Score : \n",
      "0.6422200198216056\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy : ', acc)\n",
    "\n",
    "cnf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "print('\\nPrecission Score : ', precision_score(y_test, y_pred))\n",
    "\n",
    "print('\\nRecall Score : ', recall_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('\\nF1 Score : ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We saw that 1st and 3rd approaches did well\n",
    "\n",
    "\n",
    "So combining features from the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_means = data[:,3:]\n",
    "\n",
    "# getting proper X\n",
    "y = []\n",
    "for i in range(len(X)):\n",
    "    y.append(X_means[i][0])\n",
    "\n",
    "y = np.asarray(y)\n",
    "\n",
    "X_means = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3817,)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.hstack((X_means, X))\n",
    "train_labels = Y\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=20, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C = 20)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.6740837696335078\n",
      "\n",
      "Confusion Matrix : \n",
      "[[268 121]\n",
      " [128 247]]\n",
      "\n",
      "Precission Score :  0.6711956521739131\n",
      "\n",
      "Recall Score :  0.6586666666666666\n",
      "\n",
      "F1 Score : \n",
      "0.6648721399730821\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy : ', acc)\n",
    "\n",
    "cnf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('\\nConfusion Matrix : ')\n",
    "print(cnf)\n",
    "\n",
    "print('\\nPrecission Score : ', precision_score(y_test, y_pred))\n",
    "\n",
    "print('\\nRecall Score : ', recall_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('\\nF1 Score : ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
